{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"gpuType":"T4","mount_file_id":"1z6dIWgKitXnsecI0UHseoPegnxNpf9iA","authorship_tag":"ABX9TyMH96JxszdTZ1CTudliAgZ7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# 1"],"metadata":{"id":"Fc9FurnOL--h"}},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow import keras\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","import json\n","import numpy as np\n","\n","!wget --no-check-certificate \\\n","    https://storage.googleapis.com/learning-datasets/sarcasm.json \\\n","    -O /tmp/sarcasm.json\n","\n","\n","def pad (data):\n","  return pad_sequences(data, maxlen=1000, truncating=\"post\", padding=\"post\")\n","\n","\n","training_size = 20000\n","\n","sentences = []\n","labels = []\n","\n","with open (\"/tmp/sarcasm.json\", \"r\") as f:\n","  datastore = json.load(f)\n","\n","\n","for i in datastore:\n","  sentences.append(i[\"headline\"])\n","  labels.append(i[\"is_sarcastic\"])\n","\n","training_sents = sentences[0:training_size]\n","test_sents = sentences[training_size:]\n","\n","training_labels = labels [0:training_size]\n","test_labels = labels [training_size:]\n","\n","tokenizer = Tokenizer (\n","  oov_token = \"<OOV>\",\n","  num_words = 10000\n",")\n","tokenizer.fit_on_texts(training_sents)\n","\n","wordindex = tokenizer.word_index\n","\n","training_seq = tokenizer.texts_to_sequences(training_sents)\n","pad_training_seq = pad(training_seq)\n","\n","testing_seq = tokenizer.texts_to_sequences(test_sents)\n","pad_testing_seq = pad(testing_seq)\n","\n","testing_padded = np.array(pad_testing_seq)\n","training_padded = np.array(pad_training_seq)\n","training_labels = np.array(training_labels)\n","testing_labels = np.array(test_labels)\n","\n","model = tf.keras.Sequential([\n","  tf.keras.layers.Embedding(10000, 16, input_length = 1000),\n","  tf.keras.layers.GlobalAveragePooling1D(),\n","  tf.keras.layers.Dense(24, activation=\"relu\"),\n","  tf.keras.layers.Dense(1, activation=\"sigmoid\")\n","])\n","\n","model.compile(\n","  loss = \"binary_crossentropy\",\n","  optimizer = \"adam\",\n","  metrics = [\"accuracy\"]\n",")"],"metadata":{"id":"1bddufYc-YUe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 2"],"metadata":{"id":"73NRXiqmMDVS"}},{"cell_type":"code","source":["history = model.fit(\n","  training_padded,\n","  training_labels,\n","  epochs=30,\n","  validation_data = (testing_padded, testing_labels),\n","  verbose = 2\n",")"],"metadata":{"id":"vdfXTKtKLFHc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 3"],"metadata":{"id":"ynILkV2gMLaH"}},{"cell_type":"code","source":["while True:\n","  sentence = input (\"You: \")\n","\n","  sequences = pad(tokenizer.texts_to_sequences([sentence]))\n","\n","  print(f\"{str(model.predict(sequences)[0][0])[2:4]}% sarcastic\")"],"metadata":{"id":"zqCPhFPTLKKT"},"execution_count":null,"outputs":[]}]}
